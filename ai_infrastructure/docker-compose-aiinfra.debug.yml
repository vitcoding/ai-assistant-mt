services:
  ollama-ai:
    build:
      context: .
      dockerfile: Dockerfile.ollama-intel
    container_name: ollama-ai
    ports:
      - 11434:11434
    volumes:
      - ./ollama:/root/.ollama
    # devices:
    #   - "/dev/dri/renderD128:/dev/dri/renderD128"
    #   - "/dev/dri/card1:/dev/dri/card1"
    devices:
      - /dev/dri:/dev/dri
    environment:
      - OLLAMA_DEBUG=1
      - OLLAMA_INTEL_GPU=true
    # - ONEAPI_DEVICE_SELECTOR=level_zero:gpu
    # - LIBGL_ALWAYS_SOFTWARE=0
    # group_add:
    #   - "44"
    #   - "992"
    privileged: true
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: intel
    #           capabilities: [ gpu ]
    restart: unless-stopped
    networks:
      - shared-network

  chroma-ai:
    image: chromadb/chroma
    container_name: chroma-ai
    volumes:
      - ./chroma:/chroma/chroma
    expose:
      - 8000
    ports:
      - 8010:8000
    restart: unless-stopped
    networks:
      - shared-network

  cache-ai:
    image: redis
    container_name: cache-ai
    expose:
      - ${REDIS_PORT}
    ports:
      - ${REDIS_PORT}:${REDIS_PORT}
    volumes:
      - ai-rds-data:/data
      - ai.redis.conf:/usr/local/etc/redis/redis.conf
    restart: unless-stopped
    networks:
      - shared-network

volumes:
  ai-rds-data:
  ai.redis.conf:


networks:
  shared-network:
    external: true
